{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3353127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string # for punctuation\n",
    "import pickle\n",
    "import re # for tokenization\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer # for Stemming\n",
    "ps = nltk.PorterStemmer()\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "lm = nltk.WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "# import spacy library and download english language from spacy.lang.en import English.\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    " #create an object with nlp by using English() to use english model which exist in spacy\n",
    " #use nlp to convert (token_txt) to Doc\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b404f294",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "antique = pd.read_csv(\"Antique/antiq.tsv\",sep='\\t')\n",
    "antique.columns=[\"id\",\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d43c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ازالة علامات الترقيم \n",
    "def remove_punctuation(txt):\n",
    "    txt_nopuct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopuct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d69d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_ex(text):\n",
    "    text = re.sub(r'\\b(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})\\b', r'\\3-\\1-\\2', text)\n",
    "     # remove Abbreviations\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"n't\", \"not\", text)\n",
    "    text = re.sub(r\"'re\", \"are\", text)\n",
    "    text = re.sub(r\"'s\", \"is\", text)\n",
    "    text = re.sub(r\"'d\", \"would\", text)\n",
    "    text = re.sub(r\"'ll\", \"will\", text)\n",
    "    text = re.sub(r\"'t\", \"not\", text)\n",
    "    text = re.sub(r\"'ve\", \"have\", text)\n",
    "    text = re.sub(r\"'us\", \"United State\", text)\n",
    "    text = re.sub(r\"U.S.A\", \"United State American\", text)\n",
    "\n",
    "    # Check from numbers and letters exist\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff9a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function returns a list of tokenized and stemmed words of any text\n",
    "def get_tokenized_text(txt):\n",
    "    # 1- tokens = txt.split(\" \")\n",
    "    # 2-  tokens = re.split('\\W+',txt)\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e5e1d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this fuunctin will performing stemming on tokenized words \n",
    "def word_stemmer(text):\n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed = []\n",
    "    for words in text:\n",
    "        stemmed.append(ps.stem(words))\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcf27748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function to remove stopwords from tokenized word list\n",
    "def remove_stopwords(text):\n",
    "    cleaned_text = []\n",
    "    for words in text:\n",
    "        if words not in stop_words:\n",
    "            cleaned_text.append(words)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20739bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lemmatization(token_txt):\n",
    "    # token.lemma_ : to convert every word to its main shape\n",
    "    # token.pos_: to check that the type of the main word ? [adj , noun ...]\n",
    "    # and otherwise we will use the original text for the word\n",
    "    doc = nlp(\" \".join(token_txt))\n",
    "    lemmas = [token.lemma_ if token.pos_ in [\"ADJ\", \"ADV\", \"NOUN\", \"VERB\"] else token.text for token in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c13433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Users/Lenovo/Desktop/python/AntiqueData/doc_id_list_antique.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #chech for antique document\n",
    "# print('1---remove_punctuation')\n",
    "antique['text_clean']=antique[\"text\"].apply(lambda x:remove_punctuation(x))\n",
    "pp=antique['text_clean']\n",
    "# print(pp)\n",
    "# print('2---reg_ex')\n",
    "antique['text_regex']=antique[\"text_clean\"].apply(lambda x:reg_ex(x))\n",
    "rr=antique['text_regex']\n",
    "# print(rr)\n",
    "# print('3---get_tokenized_text')\n",
    "antique['text_clean_tokenize']= antique[\"text_regex\"].apply(lambda x :get_tokenized_text(str(x).lower()))\n",
    "ff=antique['text_clean_tokenize']\n",
    "# print(ff)\n",
    "# print('4---remove_stopwords')\n",
    "antique['remove_stopwords']= antique['text_clean_tokenize'].apply(lambda x :remove_stopwords(x))\n",
    "ss=antique['remove_stopwords']\n",
    "# print(ss)\n",
    "# print('5---word_stemmer')\n",
    "antique['word_stemmer']= antique['remove_stopwords'].apply(lambda x :word_stemmer(x))\n",
    "ll=antique['word_stemmer']\n",
    "# print(ll)\n",
    "# print('5---txt_lemmatized')\n",
    "antique['txt_lemmatized']= antique['word_stemmer'].apply(lambda x:lemmatization(x))\n",
    "kk=antique['txt_lemmatized']\n",
    "# print(kk)\n",
    "joblib.dump(antique[\"txt_lemmatized\"], 'C:/Users/Lenovo/Desktop\\python/AntiqueData/antique.joblib')\n",
    "doc_id_list_antique = antique['id']\n",
    "joblib.dump(doc_id_list_antique, 'C:/Users/Lenovo/Desktop/python/AntiqueData/doc_id_list_antique.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
